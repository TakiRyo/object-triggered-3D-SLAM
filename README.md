# OTSLAM: Active Object Scanning System  

object_scan_multi_good is best. Dec. 6

## ðŸ“– System Overview  

This project implements an autonomous **"Move & Scan"** pipeline for structured 3D reconstruction. It utilizes 2D LiDAR data to detect and classify objects in the environment, tracks their persistence over time, and coordinates a mobile robot to navigate around them. Upon reaching specific viewpoints, the robot stabilizes and captures synchronized RGB-D data and camera poses for high-quality 3D mapping.

### The Pipeline

1.  **Perception (`lidar_detection`)**: Raw LiDAR data is segmented into clusters. Walls are filtered out, and compact objects are tracked.
2.  **Mission Planning (`lidar_detection`)**: The system generates **Adaptive Visiting Points** (Goals) around detected stable objects.
3.  **Orchestration (`system_manager`)**: A central manager commands the robot to drive to these points using the **Nav2** stack.
4.  **Data Capture (`system_manager`)**: Once arrived, the robot freezes its movement and triggers the camera system to capture high-quality dataset frames.

-----  

## ðŸ“¦ Package 1: Lidar Detection

*Focus: Raw Sensor Processing, Classification, and Object Tracking.*

### 1\. ðŸ§© LiDAR Cluster Classification

**Node Name:** `lidar_cluster_publisher`

Segments 2D LiDAR scans into distinct clusters and classifies them based on geometric properties (PCA linearity, size, density).

  * **Logic:**
    1.  **Clustering:** Groups points based on Euclidean distance. 
    2.  **PCA Analysis:** Calculates eigenvalues ($\lambda_0, \lambda_1$) to determine if a cluster is linear or blob-like.
    3.  **Classification:**
          * ðŸŸ© **Wall:** Long, straight, dense, high linearity.
          * ðŸŸ¦ **Object:** Short, compact (obstacles/items).
          * ðŸŸ¨ **Unknown:** Curved corners or irregular shapes.

| Topic / Parameter | Type | Description |
| :--- | :--- | :--- |
| **Sub** `/scan` | `LaserScan` | Raw LiDAR input. |
| **Pub** `/wall_clusters` | `PointCloud2` | Static environmental features. |
| **Pub** `/object_clusters` | `PointCloud2` | Candidates for tracking. |
| `wal_lin_max` | Param | Linearity threshold (Lower = stricter straight lines). |

### 2\. ðŸŽ¯ Object Tracker & Goal Generator

**Node Name:** `object_cluster_marker`

Provides temporal persistence to objects and generates navigation goals. It filters out transient noise and creates a "Lock Zone" around valid objects.

  * **Features:**
      * **State Machine:** `Candidate` (Yellow) $\to$ `Stable` (Green) if tracked for `stability_time`.

      * **Goal Generation (Adaptive Density):** Creates visiting points (Cyan Arrows) around the stable object. The quantity adapts to the object's size to ensure thorough coverage:

          * **Small/Normal Objects:** Generates **6** points.
          * **Large Objects:** Generates **8** points.
          * The transition size is based on the `scan_step_threshold` parameter comparing against the object's diagonal size.

      * **Orientation Logic:** Calculates Yaw so the robot always **faces the object center** at the destination.

      * **Service Integration:** Can "Freeze" the map state (stop updating object positions) to prevent goal jitter during navigation.

| Topic / Service | Type | Description |
| :--- | :--- | :--- |
| **Sub** `/object_clusters` | `PointCloud2` | Input from classification node. |
| **Pub** `/object_visiting_points`| `MarkerArray` | Cyan arrows representing nav goals. |
| **Srv** `set_tracking_mode` | `SetBool` | `True`=Live Lidar, `False`=Frozen Memory. |

### 3\. ðŸ“¡ Goal Sender (Mission Manager)

**Node Name:** `goal_sender`

Acts as the dispatch interface between the Tracker and the System Manager. It decides *which* point to visit next.

  * **Strategy (Greedy + Sticky):**
    1.  **Sticky Focus:** If currently visiting points belonging to "Object A", it prioritizes remaining points for "Object A" before switching focus to "Object B".
    2.  **Proximity:** If the current object's points are exhausted or no current focus exists, it picks the physically closest target from the overall queue.
    3.  **Progress Tracking:** Marks points as "Visited" when the robot gets within `reach_threshold`.

| Topic | Description |
| :--- | :--- |
| **Sub** `/object_visiting_points` | Goals generated by the Tracker. |
| **Sub** `/odom` | Robot's current position for distance calculations. |
| **Pub** `/manager/target_pose` | The active target sent to the System Manager. |
| **Pub** `/goal_status` | Visual feedback: Red (Active), Green (Done), Grey (Queue). |

-----

## ðŸ“¦ Package 2: System Manager

*Focus: Robot Control, State Machine, and Data Saving.*

### 1\. ðŸ§  System Manager

**Node Name:** `system_manager`

The central orchestrator connecting the Mission Manager, Nav2, and the Scanner. It implements a Finite State Machine: **IDLE $\to$ NAVIGATING $\to$ SCANNING**.

#### â˜… Feature Spotlight: Smart Tracking Strategy

To handle sensor noise and navigation occlusions, this node intelligently switches the Tracker's mode using the `set_tracking_mode` service:

1.  **Searching (Unfreeze):** When a **NEW** Object ID is received, Lidar tracking is enabled (`True`) to pinpoint the object's exact location before navigation begins.
2.  **Orbiting (Freeze):** When moving to a new viewpoint of the **SAME** object, tracking is **Frozen** (`False`). This prevents the center point from shifting due to changing perspective or occlusion as the robot orbits.
3.  **Scanning (Freeze):** Upon arrival, tracking is forced Frozen (`False`) to ensure coordinate stability and prevent goal jitter during data capture.

| Action / Service | Role |
| :--- | :--- |
| `Maps_to_pose` | Client for Nav2 to move the robot. |
| `scan_object` | Triggers the ScannerNode upon arrival. |
| `set_tracking_mode` | Toggles the Tracker's Lidar update loop. |

### 2\. ðŸ“¸ Scanner Node

**Node Name:** `scanner_node`

The "Photographer." Acts as an Action Server that saves a synchronized snapshot of the environment.

  * **The "Stop-and-Stare" Routine:**
    1.  **Buffer Flush:** Clears old images to ensure no motion blur from the navigation phase.
    2.  **Stabilization:** Waits for `wait_time` (e.g., 5.0s) to let robot vibrations settle.
    3.  **Capture:** Saves RGB, Depth, and TF Pose.
    4.  **Depth Conversion:** Converts raw float meters $\to$ 16-bit PNG (Millimeters) and patches $\text{NaN}$ values.
    5.  **Cool Down:** Waits again before returning success to prevent immediate robot jerking.

**Output Directory Structure:**

```text
/output_dir
â”œâ”€â”€ /color
â”‚   â””â”€â”€ label_N.jpg        # Standard RGB
â”œâ”€â”€ /depth
â”‚   â””â”€â”€ label_N.png        # 16-bit Integer Depth (mm)
â””â”€â”€ /poses
    â””â”€â”€ label_N.txt        # 4x4 Matrix (World -> Camera)
```
